{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Cross-domain product recommendation with GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improve a recommender system based on generative adversarial networks (GANs), which solely builds up on product images.\n",
    "\n",
    "Imagine you are [Zalando](#https://www.zalando.ch) and you have a customer browsing through the shoes. Then the shopper likes a pair of shoes and is going to the checkout. Before the shopper buys the shoes, you would like to show them what else you have in stock that they might like. Your product recommendation algorithm recommends a dress, so you show this to the shopper. They agree, the dress fits their style perfectly, so they throw it in the shopping cart as well! Good job!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I will be using GANs to generate cross-domain product recommendations. The two domains I will be working with are shoes and dresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A primer on generative adversarial networks (GANs)\n",
    "\n",
    "Here is a super quick introduction to GANs and CycleGANs. \n",
    "\n",
    "### GANs\n",
    "\n",
    "Even though we haven't discussed GANs in the lecture, you should be able to understand them quite quickly.\n",
    "\n",
    "GANs are designed and trained using the same tools as you learned in the lecture. A large difference is that these methods are unsupervised. That is, we don't have any natural labels. Instead, GANs use 2 neural networks that \"fight\" against eachother and learn by competing (hence \"adversarial\"). \n",
    "\n",
    "![Basic GAN architecture](imgs/GAN.png)\n",
    "\n",
    "The two networks are a generator and a discriminator. The generator network is a network that generates data. In this case, you will have a generator that generates images that belong to a category; for example, shoes. The discriminator then has to distinguish between real images of shoes and the fake images of shoes that the generator created. The generator and discriminator are trained based on their successes. The generator tries to learn to generate features that fools the discriminator. The discriminator tries to learn how it can tell fakes apart from the real thing. This is why we don't need labels!\n",
    "\n",
    "Up until now, none of this should sound too strange for you. In terms of the architecture of the neural networks, we almost only use things from the lectures. The discriminator will be a standard convolutional neural network. That is, it takes an image as input and spits out a prediction ('real' or 'fake') at the other end. The generator is a neural network that is flipped. Since the output of the generator needs to be an image, the last layer needs to produce a WxHx3 image. So rather than using downsampling convolutions that we taught in the lecture, you'll need to use upsampling convolutions. They use the same convolutional technique, but the activation maps gets wider and taller rather than smaller.\n",
    "\n",
    "### CycleGAN\n",
    "\n",
    "![CycleGAN](imgs/cyclegan.png)\n",
    "\n",
    "A CycleGAN is a special type of GAN that is used for style-transfer. It has a few features that distinguish it from other GANs that we will outline quickly. These features are:\n",
    "\n",
    "  1. 2 sets of generators and discriminators.\n",
    "  2. The input to the generator are images. \n",
    "  3. The loss is cycle-consistent.\n",
    "\n",
    "#### 1) 2 sets of generators and discriminators\n",
    "\n",
    "Since we are interested in making cross-domain recommendations, we need a generator that generates images of shoes and one that generates images of dresses. (Note, there are architectures that can generate both dresses and shoes with one generator, but they don't work well, so we will stick with 2.) Then, for each generator you will need a discriminator that distinguishes between real and fake shoes and one that distinguishes between real and fake dresses.\n",
    "\n",
    "#### 2) The input to the generator are images\n",
    "\n",
    "In the early GANs, the input to the generator was usually a vector of random noise. We used a noise vector to generate a random image. This allowed us to generate many different images in the same domain because different noise would lead to a different output. In cycleGAN, the input to the generators is an image. If we want to recommend a shoe for a given dress, we use an image of a dress as an input for the generator and generate an image of a shoe. Contrarily, if we stick an image of a shoe in the generator, we want to get an image of a dress as an output. Therefore, both the input and output of the 2 generators are images.\n",
    "\n",
    "#### 3) The loss is cycle-consistent\n",
    "\n",
    "![cycle-consistency](imgs/cycle-consistency.png)\n",
    "\n",
    "The final difference is that the loss is **cycle-consistent**. This means, that **we want the recommendations to be cyclical. If we recommend shoes X for a dress X, then it would make sense to get dress X recommended for shoe X**. You can think of this in terms of translation: if you translate a sentence from english to german and back to english, you would ideally like to get the input sentence back. The same is true for recommendations. \n",
    "\n",
    "To accomplish this we do the following: say we have a generator `G_DS` that translates dresses to shoes and a generator `G_SD` that translates shoes to dresses. Then we have an image of a dress `real_D` that we would like a shoe recommendation for. So we pass the image to the generator and get an output image of shoe: `real_D` -> `G_DS` -> `fake_S`. Then we can pass this image to the other generator to generate an image of a dress: `fake_S` -> `G_SD` -> `fake_D`. Then we compare `fake_D` to `real_D` and train the weights based on the pixel discrepancy. \n",
    "\n",
    "## Interpreting the output\n",
    "\n",
    "The losses of GAN are harder to interpret than those of normal supervised classification or regression tasks. **The best way to evaluate whether your GAN is learning is through visual inspection.**\n",
    "\n",
    "In this script, we **output figures to `output_imgs_path` generated by the GAN (see lines 213-217). The output images contain 3 images**. The first (left) is the input image to the first generator. The second image (middle) is the output of the first generator. That is, it is the translation of the first image to the second domain. The third image (right) is the the reconstuction of the first image. That is, we pass the second image to the second generator such that it translates it back to the first domain. \n",
    "\n",
    "![output-images](imgs/eg_output.png)\n",
    "\n",
    "So, how can we use these images to evaluate the training procedure? The first thing to look for is whether the second (middle) and third images (right) are recognisable. That is, can you clearly identify the images as shoes and dresses? Are the edges somewhat well defined? Does the dress have at most 2 sleeves, one neck, a body, etc.? Do the shoes have a heel, a toe, a shoe hole, etc. The next thing to look for is whether the style of the first image was translated onto the second image. Since the images are 64 by 64, there won't be a great amount of detail. But, are the colors similar? are the notible attributes translated (roughly)? **Finally, the third image should look somewhat identical to the first image**. Ideally, the third image is a reconstruction of the first, however, practically, you will never get a perfect reconstruction. But, is it somewhat similar?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "There are a few directions you can take this project:\n",
    "  \n",
    "  1. The **loss functions** in GANs are _really_ important. Can you improve the performance by tweaking or coming up with a new loss function? (Maybe Karras et al, 2017 (on dropbox) can be adapted?)  \n",
    "    \n",
    "  2. Karras et al (2017) created a great GAN called **progressive growing GAN**. The idea is that the GAN first learns to produce small images before scaling up to larger ones. So first, it learns how to generate 4x4 images. Then, once it is sufficiently good at that, it uses that as a starting point and generates 8x8 images. Then 16x16. And so on, until it can generate high-resolution images. Can you implement progressive growing in cycleGAN?    \n",
    "  \n",
    "  3. Another new method (Hicsonmez et al, 2020 (on dropbox)) called **GANILLA** just hit the market. Can you use GANILLA to generate cross-domain recommendations? How does it compare to cycleGAN? Make sure to google around before programming too much yourself (GANILLA is on GitHub).\n",
    "\n",
    "## Getting started\n",
    "\n",
    "The code presented in this notebook was taken from [here](https://github.com/aitorzip/PyTorch-CycleGAN). It offers a basic and readible implementation of the CycleGAN algorithm. You can also check [this](https://www.tensorflow.org/tutorials/generative/cyclegan) out for tensorflow code. The tensorflow code is quite good, so use whichever you feel more comfortable with. It has a few features that you won't need (e.g. crop and jitter preprocessing).\n",
    "\n",
    "To familiarize yourself with the code, you can try to implement some of the tips and tricks learned in the lectures to see if you can improve the performance. The code is very clean should be easy to adapt.\n",
    "\n",
    "A small tip before starting: make sure that the saving and loading of the weights works. I have made the experience that this API is somewhat unreliable. It will save you time if you can continue training from some checkpoint.\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "## CycleGAN for product recommendations\n",
    "\n",
    "Alright, lets get started. Import the modules you need. `Models`, `utils`, and `datasets`, which are found in `/Dropbox/.../\\#\\ Group\\ projects/Product\\ recommendations/code`, contain helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Append the path to the folder holding the scripts \n",
    "# if it is not already the working directory\n",
    "sys.path.append('.')\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "from math import floor\n",
    "\n",
    "import imageio\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from models import Generator, Discriminator\n",
    "from utils import ReplayBuffer, LambdaLR, weights_init_normal, safe_mkdirs, tensor2image, as_np\n",
    "from datasets import ImageDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can define some training parameters. Adjust these as needed. Also, feel free to add your own functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_iter = 0      # Starting iteration (if 0 train from scratch)\n",
    "n_epochs = 200     # Number of epochs of training\n",
    "batch_size = 1     # Size of the batches\n",
    "lr = 0.0002        # Initial learning rate\n",
    "decay_epoch = 100  # Epoch to start linearly decaying the learning rate to 0\n",
    "size = 128         # Size of the data crop (squared assumed)\n",
    "\n",
    "input_nc = 3       # Number of channels of input data\n",
    "output_nc = 3      # Number of channels of output data\n",
    "n_cpu = 2          # Number of cpu threads to use during batch generation\n",
    "image_size = 64    # Size of images\n",
    "\n",
    "log_interval = 200         # Interval to print output\n",
    "model_save_interval = 200  # Interval to save model weights\n",
    "image_save_interval = 200  # Interval at which to log visual progess\n",
    "\n",
    "# Paths to directories\n",
    "ROOT = '.'  # Change if necessary\n",
    "data_path = os.path.join(ROOT, 'data')\n",
    "output_path = os.path.join(ROOT, 'output_my')\n",
    "output_imgs_path = os.path.join(output_path, 'imgs')\n",
    "output_weights_path = os.path.join(output_path, 'weights')\n",
    "safe_mkdirs(output_path)\n",
    "safe_mkdirs(output_imgs_path)\n",
    "safe_mkdirs(output_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we get to the main chunk where we initialize and train the CycleGAN. Basic functionality is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Cuda if available. It should be available on colab.\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From paper: We use 6 residual blocks for 128 × 128 training images, \n",
    "# and 9 residual blocks for 256 × 256 or higher-resolution training images\n",
    "\n",
    "# in our code: defult setting in is 3\n",
    "n_res_block = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize networks\n",
    "# Domain A: dresses\n",
    "# Domain B: shoes\n",
    "netG_A2B = Generator(input_nc, output_nc,n_residual_blocks=n_res_block)\n",
    "netG_B2A = Generator(output_nc, input_nc,n_residual_blocks=n_res_block)\n",
    "netD_A = Discriminator(input_nc)\n",
    "netD_B = Discriminator(output_nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To cuda\n",
    "if use_cuda:\n",
    "    netG_A2B.cuda()\n",
    "    netG_B2A.cuda()\n",
    "    netD_A.cuda()\n",
    "    netD_B.cuda()\n",
    "\n",
    "# Initialize or load pretrained weights\n",
    "# TODO: make sure this works. I have found the save/load_state_dict API to be buggy \n",
    "# so make sure it works on your machine before you spend a lot of time\n",
    "# training a model. If you save a corrupted weights, you can't restore them.\n",
    "if load_iter == 0:\n",
    "    netG_A2B.apply(weights_init_normal)\n",
    "    netG_B2A.apply(weights_init_normal)\n",
    "    netD_A.apply(weights_init_normal)\n",
    "    netD_B.apply(weights_init_normal)\n",
    "else:\n",
    "    netG_A2B.load_state_dict(torch.load(os.path.join(output_weights_path, 'G_A2B_{}.pth'.format(load_iter))))\n",
    "    netG_B2A.load_state_dict(torch.load(os.path.join(output_weights_path, 'G_B2A_{}.pth'.format(load_iter))))\n",
    "    netD_A.load_state_dict(torch.load(os.path.join(output_weights_path, 'D_A_{}.pth'.format(load_iter))))\n",
    "    netD_B.load_state_dict(torch.load(os.path.join(output_weights_path, 'D_B_{}.pth'.format(load_iter))))\n",
    "\n",
    "    netG_A2B.train()\n",
    "    netG_B2A.train()\n",
    "    netD_A.train()\n",
    "    netD_B.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses\n",
    "# TODO: Defining the correct losses make and break the performance of GANs.\n",
    "#       Can you think of a loss that would improve the results?\n",
    "\n",
    "# Notice that there are three losses here\n",
    "# 1) criterion_GAN: This is the standard GAN loss. Whether the discriminator\n",
    "#    could correctly predict whether the image was real or fake is used to train the networks.\n",
    "#    In paper: the negative log likelihood objective is replaced by a least-squares loss. \n",
    "#    This loss is more stable during training and generates higher quality results.\n",
    "# 2) criterion_cycle: cycle-consistency discussed in the intro text.\n",
    "# 3) criterion_identity: if you put an image of a shoe into the dress to\n",
    "#    shoe generator, you should get the same image of the shoe back.\n",
    "\n",
    "# pytorch的nn.MSELoss损失函数\n",
    "# https://blog.csdn.net/hao5335156/article/details/81029791\n",
    "# PyTorch 学习笔记（六）：PyTorch的十八个损失函数\n",
    "# https://zhuanlan.zhihu.com/p/61379965\n",
    "# TORCH.NN\n",
    "# https://pytorch.org/docs/stable/nn.html\n",
    "\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_cycle = torch.nn.L1Loss()\n",
    "criterion_identity = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers & LR schedulers (with decay)\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()),\n",
    "                                lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D_A = torch.optim.Adam(netD_A.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D_B = torch.optim.Adam(netD_B.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "start_epoch = floor(load_iter/99554)\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=LambdaLR(n_epochs, start_epoch, decay_epoch).step)\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=LambdaLR(n_epochs, start_epoch, decay_epoch).step)\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=LambdaLR(n_epochs, start_epoch, decay_epoch).step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs & targets memory allocation\n",
    "Tensor = torch.cuda.FloatTensor if use_cuda else torch.Tensor\n",
    "input_A = Tensor(batch_size, input_nc, image_size, image_size)\n",
    "input_B = Tensor(batch_size, output_nc, image_size, image_size)\n",
    "target_real = Variable(Tensor(batch_size).fill_(1.0), requires_grad=False)\n",
    "target_fake = Variable(Tensor(batch_size).fill_(0.0), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize replay buffer\n",
    "# The replay buffer helps stabilize training by saving images of previous\n",
    "# iterations and using them as training data in later iterations. This helps\n",
    "# avoid over training on current data.\n",
    "\n",
    "# In paper: the default max replay buffer size is 50\n",
    "max_replay = 100\n",
    "\n",
    "fake_A_buffer = ReplayBuffer(max_size=max_replay)\n",
    "fake_B_buffer = ReplayBuffer(max_size=max_replay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orchvision中Transform的normalize参数含义\n",
    "# https://blog.csdn.net/york1996/article/details/82711593\n",
    "\n",
    "# Dataset loader\n",
    "transforms_ = [ transforms.Resize(image_size, Image.BICUBIC), \n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training set\n",
    "# The `ImageDataset` is in dataset.py. Check it out to see what it does.\n",
    "dataloader = DataLoader(ImageDataset(data_path, transforms_=transforms_, unaligned=True), \n",
    "                        batch_size=batch_size, shuffle=True, num_workers=n_cpu)\n",
    "\n",
    "# For test set\n",
    "dataloader_test = DataLoader(ImageDataset(data_path, transforms_=transforms_, mode='test'),\n",
    "                            batch_size=1, shuffle=False, num_workers=n_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch : 1\n",
      "0\n",
      "---------------------\n",
      "GAN loss: 0.12517482 0.1724944\n",
      "Identity loss: 0.4677845 0.8535755\n",
      "Cycle loss: 0.43628576 0.855853\n",
      "D loss: 2.3718777 1.1639683\n",
      "time: 1.5386250019073486\n",
      "100\n",
      "200\n",
      "---------------------\n",
      "GAN loss: 0.7022245 0.20585868\n",
      "Identity loss: 0.09330419 0.14339297\n",
      "Cycle loss: 0.100808226 0.13514327\n",
      "D loss: 0.28619793 0.37287968\n",
      "time: 283.69341015815735\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3d76c03ac11f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mloss_cycle_ABA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecovered_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mrecovered_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetG_A2B\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mloss_cycle_BAB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecovered_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/MyExercise/DL4MKT_GAN/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/torch/nn/modules/instancenorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     47\u001b[0m         return F.instance_norm(\n\u001b[1;32m     48\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             self.training or not self.track_running_stats, self.momentum, self.eps)\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36minstance_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, use_input_stats, momentum, eps)\u001b[0m\n\u001b[1;32m   1943\u001b[0m     return torch.instance_norm(\n\u001b[1;32m   1944\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1945\u001b[0;31m         \u001b[0muse_input_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1946\u001b[0m     )\n\u001b[1;32m   1947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prev_time = time.time()\n",
    "iter = load_iter\n",
    "\n",
    "# Training \n",
    "for epoch in range(start_epoch, n_epochs):\n",
    "    print(\"Current epoch :\", epoch+1)\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "        # Set model input\n",
    "        real_A = Variable(input_A.copy_(batch['A']))\n",
    "        real_B = Variable(input_B.copy_(batch['B']))\n",
    "\n",
    "        # Forward pass through each of the generators and discriminators\n",
    "        # Generators A2B and B2A ##############################################\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Identity loss\n",
    "        # G_A2B(B) should equal B if real B is fed\n",
    "        same_B = netG_A2B(real_B)\n",
    "        loss_identity_B = criterion_identity(same_B, real_B)\n",
    "        # G_B2A(A) should equal A if real A is fed\n",
    "        same_A = netG_B2A(real_A)\n",
    "        loss_identity_A = criterion_identity(same_A, real_A)\n",
    "\n",
    "        # GAN loss\n",
    "        fake_B = netG_A2B(real_A)\n",
    "        pred_fake = netD_B(fake_B)\n",
    "        loss_GAN_A2B = criterion_GAN(pred_fake, target_real)\n",
    "\n",
    "        fake_A = netG_B2A(real_B)\n",
    "        pred_fake = netD_A(fake_A)\n",
    "        loss_GAN_B2A = criterion_GAN(pred_fake, target_real)\n",
    "\n",
    "        # Cycle loss\n",
    "        recovered_A = netG_B2A(fake_B)\n",
    "        loss_cycle_ABA = criterion_cycle(recovered_A, real_A)\n",
    "\n",
    "        recovered_B = netG_A2B(fake_A)\n",
    "        loss_cycle_BAB = criterion_cycle(recovered_B, real_B)\n",
    "\n",
    "        # Total loss\n",
    "        loss_G = (loss_identity_A + loss_identity_B) * 5.0\n",
    "        loss_G += (loss_GAN_A2B + loss_GAN_B2A) * 1.0\n",
    "        loss_G += (loss_cycle_ABA + loss_cycle_BAB) * 10.0\n",
    "        \n",
    "        # Calculate the gradient of the generators\n",
    "        loss_G.backward()\n",
    "        \n",
    "        # Update the weights of the generators\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Discriminator A #####################################################\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real = netD_A(real_A)\n",
    "        loss_D_real = criterion_GAN(pred_real, target_real)\n",
    "\n",
    "        # Fake loss\n",
    "        fake_A = fake_A_buffer.push_and_pop(fake_A)\n",
    "        pred_fake = netD_A(fake_A.detach())\n",
    "        loss_D_fake = criterion_GAN(pred_fake, target_fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D_A = (loss_D_real + loss_D_fake) * 0.5\n",
    "\n",
    "        # Calculate the gradient of discriminator A\n",
    "        loss_D_A.backward()\n",
    "\n",
    "        # Update the weights of discriminator A\n",
    "        optimizer_D_A.step()\n",
    "\n",
    "        # Discriminator B #####################################################\n",
    "        optimizer_D_B.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real = netD_B(real_B)\n",
    "        loss_D_real = criterion_GAN(pred_real, target_real)\n",
    "        \n",
    "        # Fake loss\n",
    "        fake_B = fake_B_buffer.push_and_pop(fake_B)\n",
    "        pred_fake = netD_B(fake_B.detach())\n",
    "        loss_D_fake = criterion_GAN(pred_fake, target_fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D_B = (loss_D_real + loss_D_fake) * 0.5\n",
    "\n",
    "        # Calculate the gradient of discriminator B\n",
    "        loss_D_B.backward()\n",
    "\n",
    "        # Update the weights of discriminator B\n",
    "        optimizer_D_B.step()\n",
    "\n",
    "        # Track performance\n",
    "        if iter % log_interval == 0:\n",
    "            print('---------------------')\n",
    "            print('GAN loss:', as_np(loss_GAN_A2B), as_np(loss_GAN_B2A))\n",
    "            print('Identity loss:', as_np(loss_identity_A), as_np(loss_identity_B))\n",
    "            print('Cycle loss:', as_np(loss_cycle_ABA), as_np(loss_cycle_BAB))\n",
    "            print('D loss:', as_np(loss_D_A), as_np(loss_D_B))\n",
    "            print('time:', time.time() - prev_time)\n",
    "            prev_time = time.time()\n",
    "\n",
    "        # Print outputs\n",
    "        if iter % image_save_interval == 0:\n",
    "            output_path_ = os.path.join(output_imgs_path, str(iter / image_save_interval))\n",
    "            safe_mkdirs(output_path_)\n",
    "\n",
    "            for j, batch_ in enumerate(dataloader_test):\n",
    "\n",
    "                if j < 60:\n",
    "                    real_A_test = Variable(input_A.copy_(batch_['A']))\n",
    "                    real_B_test = Variable(input_B.copy_(batch_['B']))\n",
    "\n",
    "                    fake_AB_test = netG_A2B(real_A_test)\n",
    "                    fake_BA_test = netG_B2A(real_B_test)\n",
    "\n",
    "                    recovered_ABA_test = netG_B2A(fake_AB_test)\n",
    "                    recovered_BAB_test = netG_A2B(fake_BA_test)\n",
    "\n",
    "                    fn = os.path.join(output_path_, str(j))\n",
    "                    A_test = np.hstack([tensor2image(real_A_test[0]), tensor2image(fake_AB_test[0]), tensor2image(recovered_ABA_test[0])])\n",
    "                    B_test = np.hstack([tensor2image(real_B_test[0]), tensor2image(fake_BA_test[0]), tensor2image(recovered_BAB_test[0])])\n",
    "                    imageio.imwrite(fn + '_A.jpg', A_test)\n",
    "                    imageio.imwrite(fn + '_B.jpg', B_test)\n",
    "                    \n",
    "                    #imageio.imwrite(fn + '.A.jpg', tensor2image(real_A_test[0]))\n",
    "                    #imageio.imwrite(fn + '.B.jpg', tensor2image(real_B_test[0]))\n",
    "                    #imageio.imwrite(fn + '.BA.jpg', tensor2image(fake_BA_test[0]))\n",
    "                    #imageio.imwrite(fn + '.AB.jpg', tensor2image(fake_AB_test[0]))\n",
    "                    #imageio.imwrite(fn + '.ABA.jpg', tensor2image(recovered_ABA_test[0]))\n",
    "                    #imageio.imwrite(fn + '.BAB.jpg', tensor2image(recovered_BAB_test[0]))\n",
    "\n",
    "        # Save models checkpoints\n",
    "        if iter % model_save_interval == 0:\n",
    "            torch.save(netG_A2B.state_dict(), os.path.join(output_weights_path, 'netG_A2B_i{}.pth'.format(iter)))\n",
    "            torch.save(netG_B2A.state_dict(), os.path.join(output_weights_path, 'netG_B2A_i{}.pth'.format(iter)))\n",
    "            torch.save(netD_A.state_dict(), os.path.join(output_weights_path, 'netD_A_i{}.pth'.format(iter)))\n",
    "            torch.save(netD_B.state_dict(), os.path.join(output_weights_path, 'netD_B_i{}.pth'.format(iter)))\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "    # Update learning rates\n",
    "    lr_scheduler_G.step()\n",
    "    lr_scheduler_D_A.step()\n",
    "    lr_scheduler_D_B.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113219\n"
     ]
    }
   ],
   "source": [
    "k=0\n",
    "for i, batch in enumerate(dataloader):\n",
    "    k=i\n",
    "        \n",
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too much Images:   \n",
    "        train A: 98552  \n",
    "        train B: 113220  \n",
    "Should use a subset,  i.e. 10000 images per collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
