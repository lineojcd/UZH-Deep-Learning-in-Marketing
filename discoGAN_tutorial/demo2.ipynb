{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# DiscoGAN\n",
    "\n",
    "Learning to DISCOver Cross Domain Relations with GANs\n",
    "https://www.youtube.com/watch?v=9reHvktowLY\n",
    "\n",
    "![alt text](https://pbs.twimg.com/media/C7NDNRuXgAAfePz.jpg \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](http://www.aimechanic.com/wp-content/uploads/2017/03/PyTorch-DiscoGAN.png \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "Cross-domain relations are natural to us humans.\n",
    "- Suit jacket goes with dress shoes\n",
    "- english-french translation\n",
    "\n",
    "Can they be natural to machines?\n",
    "It's a conditional image generation problem. \n",
    "i.e find a mapping function from one domain to the other \n",
    "i.e generate an image in one domain given another image in the other domain. \n",
    "\n",
    "most of today’s training approaches use explicitly paired\n",
    "data, provided by human or another algorithm.\n",
    "\n",
    "Let's do it with no labels :)\n",
    "use cases\n",
    "- games\n",
    "- design with real time feedback\n",
    "\n",
    "\n",
    "take 1 image and reconstruct in the style of another\n",
    "- encoder-decoder? too naive (more like a camera filter) \n",
    "- 2 encoder-decoders? backwards compatible style transfer, but still naive\n",
    "- 2 encoder-decorers in an adversarial context? Bingo. :) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#why GMM?\n",
    "#why graph replace for discimrinator?\n",
    "#slim repeat operation\n",
    "#add 2 references\n",
    "\n",
    "\n",
    "#bridging the python 2 and python 3 gap\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os # saving files\n",
    "import numpy as np #matrix math\n",
    "\n",
    "#visualizing data\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "#machine learning\n",
    "import tensorflow as tf\n",
    "\n",
    "#gausian mixture model for generating data\n",
    "from data_gmm import GMM_distribution, sample_GMM, plot_GMM\n",
    "#analyzing data \n",
    "from data_utils import shuffle, iter_data\n",
    "\n",
    "#progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "#TF-Slim is a lightweight library for defining, training and evaluating models in TensorFlow. It enables defining complex networks quickly and concisely\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "#Classes that represent batches of statistical distributions. \n",
    "#Each class is initialized with parameters that define the distributions\n",
    "ds = tf.contrib.distributions\n",
    "\n",
    "#Create a new graph which compute the targets from the replaced Tensors.\n",
    "\n",
    "graph_replace = tf.contrib.graph_editor.graph_replace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyperparams\n",
    "\"\"\" parameters \"\"\"\n",
    "n_epoch = 1000 #number of epcohs\n",
    "batch_size  = 64\n",
    "dataset_size = 512\n",
    "input_dim = 2 #data and labels\n",
    "latent_dim = 2 \n",
    "eps_dim = 2\n",
    "\n",
    "\n",
    "#discriminator\n",
    "n_layer_disc = 2\n",
    "n_hidden_disc = 256\n",
    "\n",
    "#generator \n",
    "n_layer_gen = 2\n",
    "n_hidden_gen= 256\n",
    "\n",
    "#inference network (generator #2)\n",
    "n_layer_inf = 2\n",
    "n_hidden_inf= 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save our results to the DiscoGAN folder\n",
    "\"\"\" Create directory for results \"\"\"\n",
    "result_dir = 'results/DiscoGAN/'\n",
    "directory = result_dir\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://image.slidesharecdn.com/sampleproject-140814212447-phpapp02/95/speaker-recognition-using-gaussian-mixture-model-2-638.jpg?cb=1408051684\n",
    " \"Logo Title Text 1\")\n",
    " \n",
    "A Gaussian mixture model is a probabilistic model  that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions  with unknown parameters.\n",
    "\n",
    "![alt text](http://i.imgur.com/GJhzOUy.png \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "- X = Dataset of n elements \n",
    "- alpha = Mixing weight of the kth component. \n",
    "- sigma = Gaussian probability density function\n",
    "- mu =  Mean of the kth component.\n",
    "- sigma2 = Variance of the kth component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The demo is tested a toy dataset, \n",
    "#5-component GMM\n",
    "#A Gaussian mixture model is a probabilistic model \n",
    "#that assumes all the data points are generated from \n",
    "#a mixture of a finite number of Gaussian distributions \n",
    "#with unknown parameters.\n",
    "\n",
    "# create X dataset (first dataset)\n",
    "#applies a function to all the items in an input_list\n",
    "#lambda = anonymous functions (i.e. function that is not bound to a name)\n",
    "#creates a numpy array of 5 components\n",
    "means = map(lambda x:  np.array(x), [[0, 0],\n",
    "                                     [2, 2],\n",
    "                                     [-1, -1],\n",
    "                                     [1, -1],\n",
    "                                     [-1, 1]])\n",
    "\n",
    "#convert to list to access methods\n",
    "means = list(means)\n",
    "#standard deviation\n",
    "std = 0.1\n",
    "#variances - eye Return an identiy matrix, 2-D array with 1s on the diagonal & 0s elsewhere.\n",
    "variances = [np.eye(2) * std for _ in means]\n",
    "\n",
    "# the probability distribution that would express one's beliefs about this \n",
    "#quantity before some evidence is taken into account\n",
    "priors = [1.0/len(means) for _ in means]\n",
    "\n",
    "#create gaussian mixture model \n",
    "gaussian_mixture = GMM_distribution(means=means,\n",
    "                                               variances=variances,\n",
    "                                               priors=priors)\n",
    "\n",
    "#sample from the data using the GMM\n",
    "dataset = sample_GMM(dataset_size, means, variances, priors, sources=('features', ))\n",
    "\n",
    "#save the results\n",
    "save_path = result_dir + 'X_gmm_data.pdf'\n",
    "#plot the results\n",
    "plot_GMM(dataset, save_path)\n",
    "\n",
    "#store data and labels\n",
    "X_np_data= dataset.data['samples']\n",
    "X_labels = dataset.data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create Z dataset (second dataset)\n",
    "#2-component GMM. \n",
    "means = map(lambda x:  np.array(x), [[-1, -1],[1, 1]])\n",
    "means = list(means)\n",
    "std = 0.1\n",
    "variances = [np.eye(2) * std for _ in means]\n",
    "\n",
    "priors = [1.0/len(means) for _ in means]\n",
    "\n",
    "gaussian_mixture = GMM_distribution(means=means,\n",
    "                                               variances=variances,\n",
    "                                               priors=priors)\n",
    "dataset = sample_GMM(dataset_size, means, variances, priors, sources=('features', ))\n",
    "save_path = result_dir + 'Z_gmm_data.pdf'\n",
    "plot_GMM(dataset, save_path)\n",
    "\n",
    "Z_np_data= dataset.data['samples']\n",
    "Z_labels = dataset.data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# samples of x and z\n",
    "X_dataset = X_np_data\n",
    "Z_dataset = Z_np_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://pbs.twimg.com/media/C8QiTe2XcAATDfn.jpg \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](http://i.imgur.com/fkEbXXX.png \"Logo Title Text 1\")\n",
    ". Illustration of our models on simplified one dimensional domains. (a) ideal mapping from domain A to domain B in which the\n",
    "two domain A modes map to two different domain B modes, (b) GAN model failure case, (c) GAN with reconstruction model failure\n",
    "case.\n",
    "\n",
    "\n",
    "\n",
    "2 coupled models learn the mapping from one domain to another \n",
    "as well as  the reverse mapping for reconstruction. \n",
    "\n",
    "The two models are trained together simultaneously.\n",
    "\n",
    "- 4 generators in total\n",
    "- 2 discriminators\n",
    "\n",
    "The two generators GAB’s and the two generators\n",
    "GBA’s share parameters, and the generated images\n",
    "xBA and xAB are each fed into separate discriminators LDA\n",
    "and LDB , respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Networks \"\"\"\n",
    "\n",
    "#Each of the two coupled models learns the mapping from\n",
    "#one domain to another, and also the reverse mapping to for\n",
    "#reconstruction. The two models are trained together simultaneously.\n",
    "#The two generators GAB’s and the two generators\n",
    "#GBA’s share parameters, and the generated images\n",
    "#xBA and xAB are each fed into separate discriminators LDA\n",
    "#and LDB , respectively.\n",
    "\n",
    "\n",
    "\n",
    "#2 generators\n",
    "def generative_network(z, input_dim, n_layer, n_hidden, eps_dim):\n",
    "    with tf.variable_scope(\"generative\"):\n",
    "        h = z\n",
    "        #repeat allow us to repeatedly perform the same operation.\n",
    "        #many fully connected layers\n",
    "        h = slim.repeat(h, n_layer, slim.fully_connected, n_hidden, activation_fn=tf.nn.relu)\n",
    "        x = slim.fully_connected(h, input_dim, activation_fn=None, scope=\"p_x\")\n",
    "    return x\n",
    "\n",
    "\n",
    "def inference_network(x, latent_dim, n_layer, n_hidden, eps_dim):\n",
    "    with tf.variable_scope(\"inference\"):\n",
    "        h = x\n",
    "        h = slim.repeat(h, n_layer, slim.fully_connected, n_hidden, activation_fn=tf.nn.relu)\n",
    "        z = slim.fully_connected(h, latent_dim, activation_fn=None, scope=\"q_z\")\n",
    "    return z\n",
    "\n",
    "\n",
    "#2 discriminators\n",
    "def data_network_x(x, n_layers=2, n_hidden=256, activation_fn=None):\n",
    "    \"\"\"Approximate x log data density.\"\"\"\n",
    "    h = tf.concat(x, 1)\n",
    "    with tf.variable_scope('discriminator_x'):\n",
    "        h = slim.repeat(h, n_layers, slim.fully_connected, n_hidden, activation_fn=tf.nn.relu)\n",
    "        log_d = slim.fully_connected(h, 1, activation_fn=activation_fn)\n",
    "    return tf.squeeze(log_d, squeeze_dims=[1]) #Removes dimensions of size 1 \n",
    "    #from the shape of a tensor.\n",
    "\n",
    "\n",
    "\n",
    "def data_network_z(z, n_layers=2, n_hidden=256, activation_fn=None):\n",
    "    \"\"\"Approximate z log data density.\"\"\"\n",
    "    h = tf.concat(z, 1)\n",
    "    with tf.variable_scope('discriminator_z'):\n",
    "        h = slim.repeat(h, n_layers, slim.fully_connected, n_hidden, activation_fn=tf.nn.relu)\n",
    "        log_d = slim.fully_connected(h, 1, activation_fn=activation_fn)\n",
    "    return tf.squeeze(log_d, squeeze_dims=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Copying op: concat\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "INFO:tensorflow:Copying op: discriminator_x/Repeat/fully_connected_1/MatMul\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "INFO:tensorflow:Copying op: discriminator_x/Repeat/fully_connected_1/BiasAdd\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "INFO:tensorflow:Copying op: discriminator_x/Repeat/fully_connected_1/Relu\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "INFO:tensorflow:Copying op: discriminator_x/Repeat/fully_connected_2/MatMul\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "INFO:tensorflow:Copying op: discriminator_x/Repeat/fully_connected_2/BiasAdd\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "INFO:tensorflow:Copying op: discriminator_x/Repeat/fully_connected_2/Relu\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "INFO:tensorflow:Copying op: discriminator_x/fully_connected/MatMul\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "INFO:tensorflow:Copying op: discriminator_x/fully_connected/BiasAdd\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "INFO:tensorflow:Copying op: Squeeze\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "INFO:tensorflow:Finalizing op: concat\n",
      "INFO:tensorflow:Finalizing op: discriminator_x/Repeat/fully_connected_1/MatMul\n",
      "INFO:tensorflow:Finalizing op: discriminator_x/Repeat/fully_connected_1/BiasAdd\n",
      "INFO:tensorflow:Finalizing op: discriminator_x/Repeat/fully_connected_1/Relu\n",
      "INFO:tensorflow:Finalizing op: discriminator_x/Repeat/fully_connected_2/MatMul\n",
      "INFO:tensorflow:Finalizing op: discriminator_x/Repeat/fully_connected_2/BiasAdd\n",
      "INFO:tensorflow:Finalizing op: discriminator_x/Repeat/fully_connected_2/Relu\n",
      "INFO:tensorflow:Finalizing op: discriminator_x/fully_connected/MatMul\n",
      "INFO:tensorflow:Finalizing op: discriminator_x/fully_connected/BiasAdd\n",
      "INFO:tensorflow:Finalizing op: Squeeze\n",
      "INFO:tensorflow:Copying op: concat_2\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "INFO:tensorflow:Copying op: discriminator_z/Repeat/fully_connected_1/MatMul\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "INFO:tensorflow:Copying op: discriminator_z/Repeat/fully_connected_1/BiasAdd\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "INFO:tensorflow:Copying op: discriminator_z/Repeat/fully_connected_1/Relu\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "INFO:tensorflow:Copying op: discriminator_z/Repeat/fully_connected_2/MatMul\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "INFO:tensorflow:Copying op: discriminator_z/Repeat/fully_connected_2/BiasAdd\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "INFO:tensorflow:Copying op: discriminator_z/Repeat/fully_connected_2/Relu\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "INFO:tensorflow:Copying op: discriminator_z/fully_connected/MatMul\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "INFO:tensorflow:Copying op: discriminator_z/fully_connected/BiasAdd\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "INFO:tensorflow:Copying op: Squeeze_2\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "INFO:tensorflow:Finalizing op: concat_2\n",
      "INFO:tensorflow:Finalizing op: discriminator_z/Repeat/fully_connected_1/MatMul\n",
      "INFO:tensorflow:Finalizing op: discriminator_z/Repeat/fully_connected_1/BiasAdd\n",
      "INFO:tensorflow:Finalizing op: discriminator_z/Repeat/fully_connected_1/Relu\n",
      "INFO:tensorflow:Finalizing op: discriminator_z/Repeat/fully_connected_2/MatMul\n",
      "INFO:tensorflow:Finalizing op: discriminator_z/Repeat/fully_connected_2/BiasAdd\n",
      "INFO:tensorflow:Finalizing op: discriminator_z/Repeat/fully_connected_2/Relu\n",
      "INFO:tensorflow:Finalizing op: discriminator_z/fully_connected/MatMul\n",
      "INFO:tensorflow:Finalizing op: discriminator_z/fully_connected/BiasAdd\n",
      "INFO:tensorflow:Finalizing op: Squeeze_2\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Construct model and training ops \"\"\"\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#data1 input\n",
    "x = tf.placeholder(tf.float32, shape=(batch_size, input_dim))\n",
    "#data 2 input\n",
    "z = tf.placeholder(tf.float32, shape=(batch_size, latent_dim))\n",
    "\n",
    "# 2 generators - encoders\n",
    "p_x = generative_network(z, input_dim , n_layer_gen, n_hidden_gen, eps_dim)\n",
    "q_z = inference_network(x, latent_dim, n_layer_inf, n_hidden_inf, eps_dim)\n",
    "\n",
    "#The logit function is the inverse of the sigmoidal \"logistic\" function\n",
    "\n",
    "#2 discriminators\n",
    "decoder_logit_x = data_network_x(p_x, n_layers=n_layer_disc, n_hidden=n_hidden_disc)\n",
    "encoder_logit_x = graph_replace(decoder_logit_x, {p_x: x})\n",
    "\n",
    "decoder_logit_z = data_network_z(q_z, n_layers=n_layer_disc, n_hidden=n_hidden_disc)\n",
    "encoder_logit_z = graph_replace(decoder_logit_z, {q_z: z})\n",
    "\n",
    "#Computes softplus: log(exp(features) + 1). activation\n",
    "#for calculating loss\n",
    "encoder_sigmoid_x = tf.nn.softplus(encoder_logit_x)\n",
    "decoder_sigmoid_x = tf.nn.softplus(decoder_logit_x)\n",
    "encoder_sigmoid_z = tf.nn.softplus(encoder_logit_z)\n",
    "decoder_sigmoid_z = tf.nn.softplus(decoder_logit_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#loss functions\n",
    "\n",
    "#loss for both discriminators\n",
    "decoder_loss = decoder_sigmoid_x + decoder_sigmoid_z\n",
    "encoder_loss = encoder_sigmoid_x + encoder_sigmoid_z\n",
    "\n",
    "#combined loss for discriminators\n",
    "disc_loss = tf.reduce_mean(  encoder_loss ) - tf.reduce_mean( decoder_loss)\n",
    "\n",
    "#2 more generators (decoders)\n",
    "rec_z = inference_network(p_x, latent_dim, n_layer_inf, n_hidden_inf, eps_dim )\n",
    "rec_x = generative_network(q_z, input_dim , n_layer_gen, n_hidden_gen,  eps_dim )\n",
    "\n",
    "#compute generator loss\n",
    "#Sum of Squared Error loss\n",
    "cost_z = tf.reduce_mean(tf.pow(rec_z - z, 2))\n",
    "cost_x = tf.reduce_mean(tf.pow(rec_x - x, 2))\n",
    "#we tie in discriminator loss into generators loss\n",
    "adv_loss = tf.reduce_mean(  decoder_loss ) \n",
    "gen_loss = 1*adv_loss + 1.*cost_x  + 1.*cost_z\n",
    "\n",
    "#collect vars with names that contain this\n",
    "qvars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"inference\")\n",
    "pvars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"generative\")\n",
    "dvars_x = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"discriminator_x\")\n",
    "dvars_z = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"discriminator_z\")\n",
    "\n",
    "#use adam (gradient descent) to optimize\n",
    "opt = tf.train.AdamOptimizer(1e-4, beta1=0.5)\n",
    "\n",
    "#minimize generators loss\n",
    "train_gen_op =  opt.minimize(gen_loss, var_list=qvars + pvars)\n",
    "\n",
    "#minimize discirimaintors loss\n",
    "train_disc_op = opt.minimize(disc_loss, var_list=dvars_x + dvars_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 187/1000 [00:50<03:22,  4.02it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-386464d92975>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mf_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdisc_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_disc_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxmb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mzmb\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mf_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madv_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_z\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_gen_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxmb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mzmb\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_g\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sraval/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sraval/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sraval/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/sraval/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sraval/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" training \"\"\"\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "FG = []\n",
    "FD = []\n",
    "\n",
    "#for each epoch (log the status bar)\n",
    "for epoch in tqdm( range(n_epoch), total=n_epoch):\n",
    "    #sample from both our datasets\n",
    "    X_dataset, Z_dataset= shuffle(X_dataset, Z_dataset)\n",
    "\n",
    "    #for each x and z in our data \n",
    "    for xmb, zmb in iter_data(X_dataset, Z_dataset, size=batch_size):\n",
    "        \n",
    "        #minimize our loss functions\n",
    "        for _ in range(1):\n",
    "            f_d, _ = sess.run([disc_loss, train_disc_op], feed_dict={x: xmb, z:zmb})\n",
    "        for _ in range(5):\n",
    "            #3 components that make up generator loss\n",
    "            f_g, _ = sess.run([[adv_loss, cost_x, cost_z], train_gen_op], feed_dict={x: xmb, z:zmb})\n",
    "\n",
    "        FG.append(f_g)\n",
    "        FD.append(f_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" plot the results \"\"\"\n",
    "\n",
    "n_viz = 1\n",
    "imz = np.array([]); rmz = np.array([]); imx = np.array([]); rmx = np.array([]);\n",
    "for _ in range(n_viz):\n",
    "    for xmb, zmb in iter_data(X_np_data, Z_np_data, size=batch_size):\n",
    "        temp_imz = sess.run(q_z, feed_dict={x: xmb, z:zmb})\n",
    "        imz = np.vstack([imz, temp_imz]) if imz.size else temp_imz\n",
    "\n",
    "        temp_rmz = sess.run(rec_z, feed_dict={x: xmb, z:zmb})\n",
    "        rmz = np.vstack([rmz, temp_rmz]) if rmz.size else temp_rmz\n",
    "\n",
    "        temp_imx = sess.run(p_x, feed_dict={x: xmb, z:zmb})\n",
    "        imx = np.vstack([imx, temp_imx]) if imx.size else temp_imx\n",
    "\n",
    "        temp_rmx = sess.run(rec_x, feed_dict={x: xmb, z:zmb})\n",
    "        rmx = np.vstack([rmx, temp_rmx]) if rmx.size else temp_rmx\n",
    "\n",
    "## inferred marginal z\n",
    "fig_mz, ax = plt.subplots(nrows=1, ncols=1, figsize=(4.5, 4.5))\n",
    "ll = np.tile(X_labels, (n_viz))\n",
    "ax.scatter(imz[:, 0], imz[:, 1], c=cm.Set1(ll.astype(float)/input_dim/2.0),\n",
    "        edgecolor='none', alpha=0.5)\n",
    "ax.set_xlim(-3, 3); ax.set_ylim(-3.5, 3.5)\n",
    "ax.set_xlabel('$z_1$'); ax.set_ylabel('$z_2$')\n",
    "ax.axis('on')\n",
    "plt.savefig(result_dir + 'inferred_mz.pdf', transparent=True, bbox_inches='tight')\n",
    "\n",
    "##  reconstruced z\n",
    "fig_pz, ax = plt.subplots(nrows=1, ncols=1, figsize=(4.5, 4.5))\n",
    "ll = np.tile(Z_labels, (n_viz))\n",
    "ax.scatter(rmz[:, 0], rmz[:, 1], c=cm.Set1(ll.astype(float)/input_dim/2.0),\n",
    "           edgecolor='none', alpha=0.5)\n",
    "ax.set_xlim(-3, 3); ax.set_ylim(-3.5, 3.5)\n",
    "ax.set_xlabel('$z_1$'); ax.set_ylabel('$z_2$')\n",
    "ax.axis('on')\n",
    "plt.savefig(result_dir + 'reconstruct_mz.pdf', transparent=True, bbox_inches='tight')\n",
    "\n",
    "## inferred marginal x\n",
    "fig_pz, ax = plt.subplots(nrows=1, ncols=1, figsize=(4.5, 4.5))\n",
    "ll = np.tile(Z_labels, (n_viz))\n",
    "ax.scatter(imx[:, 0], imx[:, 1], c=cm.Set1(ll.astype(float)/input_dim/2.0),\n",
    "        edgecolor='none', alpha=0.5)\n",
    "ax.set_xlim(-3, 3); ax.set_ylim(-3.5, 3.5)\n",
    "ax.set_xlabel('$x_1$'); ax.set_ylabel('$x_2$')\n",
    "ax.axis('on')\n",
    "plt.savefig(result_dir + 'inferred_mx.pdf', transparent=True, bbox_inches='tight')\n",
    "\n",
    "##  reconstruced x\n",
    "fig_mx, ax = plt.subplots(nrows=1, ncols=1, figsize=(4.5, 4.5))\n",
    "ll = np.tile(X_labels, (n_viz))\n",
    "ax.scatter(rmx[:, 0], rmx[:, 1], c=cm.Set1(ll.astype(float)/input_dim/2.0),\n",
    "           edgecolor='none', alpha=0.5)\n",
    "ax.set_xlim(-3, 3); ax.set_ylim(-3.5, 3.5)\n",
    "ax.set_xlabel('$x_1$'); ax.set_ylabel('$x_2$')\n",
    "ax.axis('on')\n",
    "plt.savefig(result_dir + 'reconstruct_mx.pdf', transparent=True, bbox_inches='tight')\n",
    "\n",
    "## learning curves\n",
    "fig_curve, ax = plt.subplots(nrows=1, ncols=1, figsize=(4.5, 4.5))\n",
    "ax.plot(FD, label=\"Discriminator\")\n",
    "ax.plot(np.array(FG)[:,0], label=\"Generator\")\n",
    "ax.plot(np.array(FG)[:,1], label=\"Reconstruction x\")\n",
    "ax.plot(np.array(FG)[:,2], label=\"Reconstruction Z\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.xlabel('Loss')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "ax.axis('on')\n",
    "plt.savefig(result_dir + 'learning_curves.pdf', bbox_inches='tight')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
